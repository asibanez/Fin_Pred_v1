#%% Imports
import os
import pandas as pd
from glob import glob

#%% Global initialization
pd.set_option('display.max_rows', 100)

train_files = ['1996_headlines.pkl',
			   '1997_headlines.pkl',
			   '1998_headlines.pkl',
			   '1999_headlines.pkl',
			   '2000_headlines.pkl',
			   '2001_headlines.pkl',
			   '2002_headlines.pkl',
			   '2003_headlines.pkl',
			   '2004_headlines.pkl',
			   '2005_headlines.pkl',
			   '2006_headlines.pkl',
			   '2007_headlines.pkl',
			   '2008_headlines.pkl',
			   '2009_headlines.pkl',
			   '2010_headlines.pkl',
			   '2011_headlines.pkl',
			   '2012_headlines.pkl',
			   '2013_headlines.pkl',
			   '2014_headlines.pkl']

dev_files = ['2015_headlines.pkl',
			 '2016_headlines.pkl',
			 '2017_headlines.pkl',
			 '2018_headlines.pkl']

test_files = ['2019_headlines.pkl',
 			  '2020_headlines.pkl',
			  '2021_headlines.pkl']

model_train = pd.DataFrame()
model_dev = pd.DataFrame()
model_test = pd.DataFrame()

#%% Path definitions
input_folder = '/home/sibanez/Projects/MyInvestor/NLP/01_spyproject/00_data/00_raw/01_full'
output_folder = '/home/sibanez/Projects/MyInvestor/NLP/01_spyproject/00_data/02_preprocessed/02_ProsusAI_finbert/00_binary/01_FULL'

output_path_train = os.path.join(output_folder, 'model_train.pkl')
output_path_dev = os.path.join(output_folder, 'model_dev.pkl')
output_path_test = os.path.join(output_folder, 'model_test.pkl')

#%% Load dataset
for file in train_files:
	data_aux = pd.read_pickle(file)




dataset_df = pd.read_pickle(input_path)

#%% Slice, remove invalid values and rename columns
X_values = ['token_ids_headline',
            'token_types_headline',
            'att_masks_headline']
#%%
Y_value = 'labels_p1'
output_df = dataset_df[X_values + [Y_value]]
slicer = dataset_df[Y_value] == 'NaN'
output_df = output_df[~slicer]
output_df.columns = ['token_ids',
                     'token_types',
                     'att_masks',
                     'Y']

#%% Check dataset size
print(f'Shape dataset before slicing = {dataset_df.shape}')
print(f'Shape dataset after slicing = {output_df.shape}')

#%% Split datasets
train_set_df, test_dev_set_df = train_test_split(output_df, test_size = 0.2)
dev_set_df, test_set_df = train_test_split(test_dev_set_df, test_size = 0.5)

#%% Check dataset sizes
print(f'\nShape train set = {train_set_df.shape}')
print(f'% train set: {len(train_set_df)/len(output_df)*100:.2f}%\n')
print(f'Shape dev set = {dev_set_df.shape}')
print(f'Dev: {len(dev_set_df)/len(output_df)*100:.2f}%\n')
print(f'Shape test set = {test_set_df.shape}')
print(f'Test: {len(test_set_df)/len(output_df)*100:.2f}%\n')

#%% Save datasets
train_set_df.to_pickle(output_path_train)
dev_set_df.to_pickle(output_path_dev)
test_set_df.to_pickle(output_path_test)
